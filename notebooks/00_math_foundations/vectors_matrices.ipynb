{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# \ud83d\udcd0 Vectors & Matrices: The Engine of Machine Learning\n",
                "\n",
                "> **\"God used beautiful mathematics in creating the world.\"** - Paul Dirac\n",
                "\n",
                "Welcome to the **GOAT (Greatest Of All Time)** guide to Linear Algebra for Machine Learning. \n",
                "\n",
                "This is not just code. This is **theory**, **intuition**, and **implementation** woven together. We will explore the mathematical objects that power everything from simple regressions to Large Language Models.\n",
                "\n",
                "## \ud83e\udde0 What You Will Learn\n",
                "1.  **Vectors**: The atoms of computation.\n",
                "2.  **Norms**: How to measure \"size\" and \"distance\" in high dimensions.\n",
                "3.  **Dot Products**: The geometry of similarity and projection.\n",
                "4.  **Matrices**: Linear transformations that warp space.\n",
                "5.  **Matrix Decompositions**: X-raying matrices to see their DNA (Eigendecomposition & SVD).\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "\n",
                "# Visual Styling\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_context(\"notebook\", font_scale=1.2)\n",
                "%matplotlib inline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# 1. Vectors: Arrows in Space\n",
                "\n",
                "## 1.1 Mathematical Definition\n",
                "A vector $\\mathbf{v}$ in an $n$-dimensional Euclidean space $\\mathbb{R}^n$ is an ordered list of $n$ real numbers.\n",
                "\n",
                "$$\n",
                "\\mathbf{v} = \\begin{bmatrix}\n",
                "v_1 \\\\\n",
                "v_2 \\\\\n",
                "\\vdots \\\\\n",
                "v_n\n",
                "\\end{bmatrix} \\in \\mathbb{R}^n\n",
                "$$\n",
                "\n",
                "## \ud83d\udca1 Intuition\n",
                "*   **Physics**: An arrow with length (magnitude) and direction.\n",
                "*   **CS**: An array of data points (e.g., a row in a database).\n",
                "*   **ML**: A single example or \"feature vector\" representing an object (e.g., pixel values of an image).\n",
                "\n",
                "Let's create some vectors in $\\mathbb{R}^3$.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Defining vectors in R3\n",
                "v1 = np.array([2, 5, 1])\n",
                "v2 = np.array([1, -3, 4])\n",
                "\n",
                "print(f\"Vector v1: {v1} | Shape: {v1.shape}\")\n",
                "print(f\"Vector v2: {v2} | Shape: {v2.shape}\")\n",
                "\n",
                "# In ML, we often treat 1D arrays as Row vectors or Column vectors depending on context.\n",
                "# Explicit column vector:\n",
                "v_col = v1.reshape(-1, 1)\n",
                "print(f\"\\nColumn Vector v1 form:\\n{v_col}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 1.2 Vector Arithmetic\n",
                "\n",
                "### Vector Addition\n",
                "Geometric interpretation: **Tail-to-Head** rule.\n",
                "Algebraic definition: Element-wise addition.\n",
                "\n",
                "$$\n",
                "\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} a_1 + b_1 \\\\ \\vdots \\\\ a_n + b_n \\end{bmatrix}\n",
                "$$\n",
                "\n",
                "### Scalar Multiplication\n",
                "Scaling a vector by a real number $\\alpha \\in \\mathbb{R}$. This stretches or shrinks the vector (and reverses it if $\\alpha < 0$).\n",
                "\n",
                "$$\n",
                "\\alpha \\mathbf{v} = \\begin{bmatrix} \\alpha v_1 \\\\ \\vdots \\\\ \\alpha v_n \\end{bmatrix}\n",
                "$$\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Addition\n",
                "v_sum = v1 + v2\n",
                "print(f\"v1 + v2 = {v_sum}\")\n",
                "\n",
                "# Scalar Multiplication\n",
                "alpha = 3.5\n",
                "v_scaled = alpha * v1\n",
                "print(f\"{alpha} * v1 = {v_scaled}\")\n",
                "\n",
                "# Linear Combination: The most important operation!\n",
                "# y = alpha*v1 + beta*v2\n",
                "beta = -2.0\n",
                "lin_comb = alpha * v1 + beta * v2\n",
                "print(f\"Linear Combination ({alpha}*v1 + {beta}*v2) = {lin_comb}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# 2. The Dot Product: The Metric of Similarity\n",
                "\n",
                "The dot product is arguably the **single most important operation** in Neural Networks (e.g., Convolution, Attention mechanisms).\n",
                "\n",
                "## 2.1 Algebraic Definition\n",
                "The sum of element-wise products.\n",
                "\n",
                "$$\n",
                "\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^T \\mathbf{b} = \\sum_{i=1}^n a_i b_i\n",
                "$$\n",
                "\n",
                "## 2.2 Geometric Definition\n",
                "The product of magnitudes and the cosine of the angle $\\theta$ between them.\n",
                "\n",
                "$$\n",
                "\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n",
                "$$\n",
                "\n",
                "## \ud83d\udca1 Why it matters\n",
                "*   **Similarity**: If $\\mathbf{a}$ and $\\mathbf{b}$ point in the same direction, dot product is positive (Large). If orthogonal ($90^\\circ$), it is **Zero**. If opposite, it is negative.\n",
                "*   **Projections**: It tells us how much of one vector \"goes along\" the other.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dot Product two ways\n",
                "dot_alg = np.sum(v1 * v2)        # Manual element-wise sum\n",
                "dot_numpy = np.dot(v1, v2)       # Typical numpy way\n",
                "dot_op = v1 @ v2                 # The @ operator (Python 3.5+) behavior for 1D is dot product\n",
                "\n",
                "print(f\"Dot Product (Manual): {dot_alg}\")\n",
                "print(f\"Dot Product (NumPy):  {dot_numpy}\")\n",
                "print(f\"Dot Product (@ op):   {dot_op}\")\n",
                "\n",
                "# Orthogonality Check\n",
                "v_ortho_1 = np.array([1, 0])\n",
                "v_ortho_2 = np.array([0, 1])\n",
                "print(f\"Dot product of orthogonal vectors: {v_ortho_1 @ v_ortho_2}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# 3. Vector Norms: Measuring \"Size\"\n",
                "\n",
                "How big is a vector? In ML, we care about the \"length\" or \"magnitude\" of error vectors (Loss functions) or weight vectors (Regularization).\n",
                "\n",
                "## General $L_p$ Norm\n",
                "$$\n",
                "\\|\\mathbf{x}\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}\n",
                "$$\n",
                "\n",
                "### 3.1 $L_2$ Norm (Euclidean Norm)\n",
                "Physical distance \"as the crow flies\".\n",
                "$$\n",
                "\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum x_i^2}\n",
                "$$\n",
                "\n",
                "### 3.2 $L_1$ Norm (Manhattan Norm)\n",
                "Distance traveling along grid lines. Promotes **sparsity** in ML (Lasso Regularization).\n",
                "$$\n",
                "\\|\\mathbf{x}\\|_1 = \\sum |x_i|\n",
                "$$\n",
                "\n",
                "### 3.3 $L_\\infty$ Norm (Max Norm)\n",
                "The maximum component.\n",
                "$$\n",
                "\\|\\mathbf{x}\\|_\\infty = \\max(|x_i|)\n",
                "$$\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate Norms\n",
                "v = np.array([3, -4])\n",
                "\n",
                "# L2 Norm (Euclidean)\n",
                "norm_l2 = np.linalg.norm(v)\n",
                "print(f\"Vector v: {v}\")\n",
                "print(f\"L2 Norm: {norm_l2}  (Should be 5.0 for [3, -4])\")\n",
                "\n",
                "# L1 Norm (Manhattan)\n",
                "norm_l1 = np.linalg.norm(v, 1)\n",
                "print(f\"L1 Norm: {norm_l1}  (Should be 3+4=7)\")\n",
                "\n",
                "# L-inf Norm (Max)\n",
                "norm_linf = np.linalg.norm(v, np.inf)\n",
                "print(f\"L-inf Norm: {norm_linf} (Should be 4)\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# 4. Matrices: Linear Transformations\n",
                "\n",
                "A matrix is a 2D array of numbers. But conceptually...\n",
                "**A Matrix IS a function.**\n",
                "$$ f(\\mathbf{x}) = A\\mathbf{x} $$\n",
                "It takes a vector input and transforms it (rotates, scales, shears, projects) into a new vector.\n",
                "\n",
                "$$\n",
                "A \\in \\mathbb{R}^{m \\times n}\n",
                "$$\n",
                "$m$ rows, $n$ columns. Maps $\\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
                "\n",
                "## 4.1 Operations\n",
                "*   **Transpose ($A^T$)**: Flip rows and columns.\n",
                "*   **Addition**: Element-wise.\n",
                "*   **Multiplication (Matrix Product)**: NOT element-wise!\n",
                "    $$ C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_k A_{ik} B_{kj} $$\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([\n",
                "    [1, 2],\n",
                "    [3, 4],\n",
                "    [5, 6]\n",
                "]) # 3x2 Matrix\n",
                "\n",
                "B = np.array([\n",
                "    [7, 8, 9],\n",
                "    [10, 11, 12]\n",
                "]) # 2x3 Matrix\n",
                "\n",
                "print(f\"Matrix A (3x2):\\n{A}\")\n",
                "print(f\"Matrix B (2x3):\\n{B}\")\n",
                "\n",
                "# Transpose\n",
                "print(f\"A Transposed (2x3):\\n{A.T}\")\n",
                "\n",
                "# Matrix Multiplication (3x2) @ (2x3) -> (3x3)\n",
                "C = A @ B  # Preferred syntax in modern Python\n",
                "print(f\"A @ B (3x3):\\n{C}\")\n",
                "\n",
                "# Note: B @ A is (2x3) @ (3x2) -> (2x2)\n",
                "print(f\"B @ A (2x2):\\n{B @ A}\")\n",
                "print(\"Note: Matrix Multiplication is NON-COMMUTATIVE! AB != BA\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## 5. Matrix Properties\n",
                "\n",
                "### 5.1 Identity Matrix ($I$)\n",
                "The \"do nothing\" transformation. $I\\mathbf{x} = \\mathbf{x}$.\n",
                "Square matrix with 1s on diagonal, 0s elsewhere.\n",
                "\n",
                "### 5.2 Inverse Matrix ($A^{-1}$)\n",
                "Reverses the transformation of $A$.\n",
                "$$ A A^{-1} = I $$\n",
                "*   **Singular Matrix**: A matrix that has **no** inverse (Determinant is 0). It \"squishes\" space into a lower dimension, destroying information.\n",
                "\n",
                "### 5.3 Determinant ($\\text{det}(A)$)\n",
                "The geometric **scaling factor** of the linear transformation.\n",
                "*   If $\\text{det}(A) = 2$, the matrix doubles the area/volume.\n",
                "*   If $\\text{det}(A) = 0$, the matrix squashes the volume to zero (flat).\n",
                "*   If $\\text{det}(A) < 0$, the matrix flips orientation (like a mirror).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Square matrix for determinant/inverse examples\n",
                "M = np.array([\n",
                "    [1, 2],\n",
                "    [3, 4]\n",
                "])\n",
                "\n",
                "det_M = np.linalg.det(M)\n",
                "print(f\"Matrix M:\\n{M}\")\n",
                "print(f\"Determinant of M: {det_M:.2f} (1*4 - 2*3 = -2)\")\n",
                "\n",
                "inv_M = np.linalg.inv(M)\n",
                "print(f\"Inverse of M:\\n{inv_M}\")\n",
                "\n",
                "# Verify Intersection\n",
                "identity_check = M @ inv_M\n",
                "print(f\"M @ M_inv (Should be Identity):\\n{np.round(identity_check, 2)}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# 6. The \"DNA\" of Matrices: Eigenvectors & SVD\n",
                "\n",
                "This is where Linear Algebra becomes magic in Machine Learning (PCA, Compression, Google PageRank).\n",
                "\n",
                "## 6.1 Eigenvalues and Eigenvectors\n",
                "For a square matrix $A$, an **eigenvector** $\\mathbf{v}$ is a vector that does *not* rotate when $A$ is applied, it only scales.\n",
                "\n",
                "$$ A \\mathbf{v} = \\lambda \\mathbf{v} $$\n",
                "\n",
                "*   $\\mathbf{v}$: Eigenvector (The conceptual \"axis\" of rotation/scaling)\n",
                "*   $\\lambda$: Eigenvalue (How much we stretch along that axis)\n",
                "\n",
                "## 6.2 Singular Value Decomposition (SVD)\n",
                "The General Theory for **ALL** matrices (not just square ones). \n",
                "Any matrix $A$ can be decomposed into:\n",
                "\n",
                "$$ A = U \\Sigma V^T $$\n",
                "\n",
                "1.  **$V^T$**: Rotation in domain.\n",
                "2.  **$\\Sigma$**: Scaling (stretching/shrinking).\n",
                "3.  **$U$**: Rotation in co-domain.\n",
                "\n",
                "This allows us to break down complex data transformations into simple, understandable principal components.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Eigendecomposition\n",
                "Sym = np.array([\n",
                "    [4, 2],\n",
                "    [2, 3]\n",
                "])\n",
                "eigenvals, eigenvecs = np.linalg.eig(Sym)\n",
                "\n",
                "print(f\"Symmetric Matrix:\\n{Sym}\")\n",
                "print(f\"Eigenvalues: {eigenvals}\")\n",
                "print(f\"Eigenvectors:\\n{eigenvecs}\")\n",
                "\n",
                "# SVD Example\n",
                "vals = np.array([\n",
                "    [1, 2, 3],\n",
                "    [4, 5, 6]\n",
                "])\n",
                "U, S, Vt = np.linalg.svd(vals)\n",
                "\n",
                "print(f\"\\nMatrix for SVD (2x3):\\n{vals}\")\n",
                "print(f\"U (Left Singular): {U.shape}\\n{U}\")\n",
                "print(f\"S (Singular Values): {S}\")\n",
                "print(f\"V.T (Right Singular): {Vt.shape}\\n{Vt}\")\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}