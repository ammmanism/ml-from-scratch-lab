{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07_calculus_derivatives.ipynb\n",
    "\n",
    "## From First Principles: Calculus and Derivatives for Machine Learning\n",
    "\n",
    "This notebook covers fundamental calculus concepts essential for machine learning, focusing on derivatives, gradients, and their applications in optimization. We'll implement key concepts from scratch and demonstrate their relevance to ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Derivatives\n",
    "\n",
    "The derivative of a function represents the rate of change of the function with respect to its input. It's fundamental to optimization in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x, h=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate numerical derivative using the limit definition:\n",
    "    f'(x) = lim(h->0) [f(x+h) - f(x-h)] / (2h)\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Example: f(x) = x^2\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Analytical derivative: f'(x) = 2x\n",
    "def f_prime_analytical(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Test at several points\n",
    "test_points = [0, 1, 2, -1, -2]\n",
    "print(\"Comparing numerical vs analytical derivatives for f(x) = x^2\")\n",
    "print(\"x\\tNumerical\\tAnalytical\\tError\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for x in test_points:\n",
    "    numerical = numerical_derivative(f, x)\n",
    "    analytical = f_prime_analytical(x)\n",
    "    error = abs(numerical - analytical)\n",
    "    print(f\"{x}\\t{numerical:.6f}\\t{analytical:.6f}\\t{error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Derivative Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate common derivative rules\n",
    "\n",
    "# Constant rule: d/dx[c] = 0\n",
    "def constant_func(x):\n",
    "    return 5\n",
    "\n",
    "print(f\"Derivative of f(x) = 5 at x=2: {numerical_derivative(constant_func, 2):.2e} (should be ~0)\")\n",
    "\n",
    "# Power rule: d/dx[x^n] = n*x^(n-1)\n",
    "def power_func(x):\n",
    "    return x ** 3\n",
    "\n",
    "def power_func_derivative(x):\n",
    "    return 3 * x ** 2\n",
    "\n",
    "x = 2\n",
    "num_deriv = numerical_derivative(power_func, x)\n",
    "ana_deriv = power_func_derivative(x)\n",
    "print(f\"Derivative of f(x) = x^3 at x=2: Numerical={num_deriv:.4f}, Analytical={ana_deriv:.4f}\")\n",
    "\n",
    "# Sum rule: d/dx[f(x) + g(x)] = f'(x) + g'(x)\n",
    "def sum_func(x):\n",
    "    return x**2 + 3*x\n",
    "\n",
    "def sum_func_derivative(x):\n",
    "    return 2*x + 3\n",
    "\n",
    "x = 1\n",
    "num_deriv = numerical_derivative(sum_func, x)\n",
    "ana_deriv = sum_func_derivative(x)\n",
    "print(f\"Derivative of f(x) = x^2 + 3x at x=1: Numerical={num_deriv:.4f}, Analytical={ana_deriv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partial Derivatives\n",
    "\n",
    "For functions of multiple variables, partial derivatives measure the rate of change with respect to one variable while holding others constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative(f, point, var_idx, h=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate partial derivative of multivariable function f at point\n",
    "    with respect to variable at index var_idx\n",
    "    \"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    point_plus = point.copy()\n",
    "    point_minus = point.copy()\n",
    "    \n",
    "    point_plus[var_idx] += h\n",
    "    point_minus[var_idx] -= h\n",
    "    \n",
    "    return (f(point_plus) - f(point_minus)) / (2 * h)\n",
    "\n",
    "# Example: f(x, y) = x^2 + 2*y^2 + x*y\n",
    "def f_xy(vars):\n",
    "    x, y = vars\n",
    "    return x**2 + 2*y**2 + x*y\n",
    "\n",
    "# Analytical partial derivatives:\n",
    "# ∂f/∂x = 2x + y\n",
    "# ∂f/∂y = 4y + x\n",
    "\n",
    "point = [1, 2]\n",
    "x, y = point\n",
    "\n",
    "print(f\"Function: f(x,y) = x^2 + 2*y^2 + x*y\")\n",
    "print(f\"At point ({x}, {y}):\")\n",
    "\n",
    "# Calculate partial derivatives numerically\n",
    "partial_x_num = partial_derivative(f_xy, point, 0)\n",
    "partial_y_num = partial_derivative(f_xy, point, 1)\n",
    "\n",
    "# Calculate analytically\n",
    "partial_x_ana = 2*x + y\n",
    "partial_y_ana = 4*y + x\n",
    "\n",
    "print(f\"∂f/∂x - Numerical: {partial_x_num:.6f}, Analytical: {partial_x_ana:.6f}\")\n",
    "print(f\"∂f/∂y - Numerical: {partial_y_num:.6f}, Analytical: {partial_y_ana:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients\n",
    "\n",
    "The gradient is a vector of all partial derivatives of a multivariable function. It points in the direction of steepest ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, point, h=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate gradient of function f at point\n",
    "    \"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    grad = np.zeros_like(point)\n",
    "    \n",
    "    for i in range(len(point)):\n",
    "        grad[i] = partial_derivative(f, point, i, h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# Example: f(x, y, z) = x^2 + y^2 + z^2 + 2*x*y*z\n",
    "def f_xyz(vars):\n",
    "    x, y, z = vars\n",
    "    return x**2 + y**2 + z**2 + 2*x*y*z\n",
    "\n",
    "# Analytical gradient:\n",
    "# ∇f = [2x + 2yz, 2y + 2xz, 2z + 2xy]\n",
    "def grad_f_xyz_analytical(vars):\n",
    "    x, y, z = vars\n",
    "    return np.array([2*x + 2*y*z, 2*y + 2*x*z, 2*z + 2*x*y])\n",
    "\n",
    "point = [1, 2, 3]\n",
    "numerical_grad = gradient(f_xyz, point)\n",
    "analytical_grad = grad_f_xyz_analytical(point)\n",
    "\n",
    "print(f\"Function: f(x,y,z) = x^2 + y^2 + z^2 + 2*x*y*z\")\n",
    "print(f\"At point {point}:\")\n",
    "print(f\"Numerical gradient: {numerical_grad}\")\n",
    "print(f\"Analytical gradient: {analytical_grad}\")\n",
    "print(f\"Difference: {np.linalg.norm(numerical_grad - analytical_grad):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Chain Rule\n",
    "\n",
    "The chain rule is crucial for computing derivatives of composite functions, which is essential in neural networks (backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: f(g(x)) where g(x) = x^2 and f(u) = sin(u)\n",
    "# So we have sin(x^2)\n",
    "# By chain rule: d/dx[sin(x^2)] = cos(x^2) * 2x\n",
    "\n",
    "def composite_function(x):\n",
    "    return np.sin(x**2)\n",
    "\n",
    "def composite_derivative_analytical(x):\n",
    "    return np.cos(x**2) * 2*x\n",
    "\n",
    "# Test at several points\n",
    "test_points = [0, 0.5, 1, 1.5, 2]\n",
    "print(\"Chain rule verification for f(x) = sin(x^2)\")\n",
    "print(\"x\\tNumerical\\tAnalytical\\tError\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for x in test_points:\n",
    "    numerical = numerical_derivative(composite_function, x)\n",
    "    analytical = composite_derivative_analytical(x)\n",
    "    error = abs(numerical - analytical)\n",
    "    print(f\"{x}\\t{numerical:.6f}\\t{analytical:.6f}\\t{error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization: Finding Minima and Maxima\n",
    "\n",
    "In machine learning, we often want to minimize loss functions. Critical points occur where the gradient is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: f(x) = x^4 - 4x^3 + 6x^2 - 4x + 1\n",
    "# f'(x) = 4x^3 - 12x^2 + 12x - 4\n",
    "# Find critical points by solving f'(x) = 0\n",
    "\n",
    "def f_optimize(x):\n",
    "    return x**4 - 4*x**3 + 6*x**2 - 4*x + 1\n",
    "\n",
    "def f_prime_optimize(x):\n",
    "    return 4*x**3 - 12*x**2 + 12*x - 4\n",
    "\n",
    "# Plot the function and its derivative\n",
    "x = np.linspace(-1, 4, 1000)\n",
    "y = [f_optimize(xi) for xi in x]\n",
    "dy = [f_prime_optimize(xi) for xi in x]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, label='f(x) = x^4 - 4x^3 + 6x^2 - 4x + 1', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Function to Optimize')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, dy, 'r-', label=\"f'(x) = 4x^3 - 12x^2 + 12x - 4\", linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title(\"Derivative (Find where f'(x) = 0)\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Let's find the minimum using gradient descent\n",
    "def gradient_descent(func, grad_func, start_x, learning_rate=0.01, iterations=1000, tolerance=1e-6):\n",
    "    \"\"\"Simple gradient descent implementation\"\"\"\n",
    "    x = start_x\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        grad = grad_func(x)\n",
    "        new_x = x - learning_rate * grad\n",
    "        \n",
    "        if abs(new_x - x) < tolerance:\n",
    "            print(f\"Converged after {i+1} iterations\")\n",
    "            break\n",
    "            \n", \n",
    "        x = new_x\n",
    "        history.append(x)\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Run gradient descent starting from different points\n",
    "start_points = [-0.5, 0.5, 2, 3]\n",
    "print(\"Gradient descent results:\")\n",
    "for start in start_points:\n",
    "    min_x, history = gradient_descent(f_optimize, f_prime_optimize, start)\n",
    "    min_val = f_optimize(min_x)\n",
    "    print(f\"Start: {start:4.1f} -> Min at x={min_x:6.4f}, f(x)={min_val:8.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applications in Machine Learning: Linear Regression\n",
    "\n",
    "Let's see how derivatives are used in optimizing a simple machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression: minimize MSE loss\n",
    "# Loss = (1/2m) * sum((y_pred - y_true)^2)\n",
    "# where y_pred = wx + b\n",
    "\n",
    "def mse_loss(w, b, X, y):\n",
    "    \"\"\"Mean Squared Error loss\"\"\"\n",
    "    predictions = w * X + b\n",
    "    return np.mean((predictions - y) ** 2) / 2\n",
    "\n",
    "def mse_loss_gradient(w, b, X, y):\n",
    "    \"\"\"Gradient of MSE loss with respect to w and b\"\"\"\n",
    "    m = len(X)\n",
    "    predictions = w * X + b\n",
    "    \n",
    "    # Partial derivative with respect to w\n",
    "    dw = (1/m) * np.sum((predictions - y) * X)\n",
    "    # Partial derivative with respect to b\n",
    "    db = (1/m) * np.sum(predictions - y)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X.flatten() + np.random.randn(100)\n",
    "\n",
    "# Initialize parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "learning_rate = 0.1\n",
    "iterations = 1000\n",
    "\n",
    "print(\"Linear Regression with Gradient Descent\")\n",
    "print(f\"True parameters: w=3, b=4\")\n",
    "print(f\"Initial parameters: w={w}, b={b}\")\n",
    "\n",
    "# Gradient descent\n",
    "w_history = [w]\n",
    "b_history = [b]\n",
    "loss_history = [mse_loss(w, b, X.flatten(), y)]\n",
    "\n",
    "for i in range(iterations):\n",
    "    dw, db = mse_loss_gradient(w, b, X.flatten(), y)\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    loss_history.append(mse_loss(w, b, X.flatten(), y))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: w={w:.4f}, b={b:.4f}, loss={loss_history[-1]:.6f}\")\n",
    "\n",
    "print(f\"Final parameters: w={w:.4f}, b={b:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Data and fitted line\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, alpha=0.5, label='Data')\n",
    "plt.plot(X, w*X.flatten() + b, 'r-', label=f'Fitted: y={w:.2f}x + {b:.2f}')\n",
    "plt.plot(X, 4 + 3*X.flatten(), 'g--', label='True: y=3x + 4')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Loss over iterations\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Parameter evolution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(w_history, label='w (slope)', linewidth=2)\n",
    "plt.plot(b_history, label='b (intercept)', linewidth=2)\n",
    "plt.axhline(y=3, color='r', linestyle='--', label='True w=3')\n",
    "plt.axhline(y=4, color='g', linestyle='--', label='True b=4')\n",
    "plt.title('Parameter Evolution')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Higher-Order Derivatives\n",
    "\n",
    "Second derivatives tell us about the curvature of a function, which is useful for determining if a critical point is a minimum, maximum, or saddle point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_derivative(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate second derivative using central difference formula\n",
    "    f''(x) ≈ [f(x+h) - 2*f(x) + f(x-h)] / h^2\n",
    "    \"\"\"\n",
    "    return (f(x + h) - 2*f(x) + f(x - h)) / (h**2)\n",
    "\n",
    "# Example: f(x) = x^4 - 4x^3 + 6x^2 - 4x + 1\n",
    "# f'(x) = 4x^3 - 12x^2 + 12x - 4\n",
    "# f''(x) = 12x^2 - 24x + 12\n",
    "\n",
    "def f_double_prime_analytical(x):\n",
    "    return 12*x**2 - 24*x + 12\n",
    "\n",
    "# Test at several points\n",
    "test_points = [0, 1, 2]\n",
    "print(\"Second derivative verification for f(x) = x^4 - 4x^3 + 6x^2 - 4x + 1\")\n",
    "print(\"x\\tNumerical\\tAnalytical\\tError\\tInterpretation\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for x in test_points:\n",
    "    numerical = second_derivative(f_optimize, x)\n",
    "    analytical = f_double_prime_analytical(x)\n",
    "    error = abs(numerical - analytical)\n",
    "    \n",
    "    # Determine nature of critical point (if f'(x) ≈ 0)\n",
    "    first_deriv = f_prime_optimize(x)\n",
    "    if abs(first_deriv) < 1e-5:  # Critical point\n",
    "        if analytical > 0:\n",
    "            nature = \"Minimum\"\n",
    "        elif analytical < 0:\n",
    "            nature = \"Maximum\"\n",
    "        else:\n",
    "            nature = \"Inflection/Saddle\"\n",
    "    else:\n",
    "        nature = \"Not critical\"\n",
    "    \n",
    "    print(f\"{x}\\t{numerical:.6f}\\t{analytical:.6f}\\t{error:.2e}\\t{nature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Jacobian and Hessian Matrices\n",
    "\n",
    "For vector-valued functions and optimization of multivariable functions, we need the Jacobian and Hessian matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_vector_func(funcs, point, h=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate Jacobian matrix for a vector-valued function\n",
    "    J[i,j] = ∂f_i/∂x_j\n",
    "    \"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    n_inputs = len(point)\n",
    "    n_outputs = len(funcs(point)) if hasattr(funcs(point), '__len__') else 1\n",
    "    \n",
    "    if n_outputs == 1:  # Single output, return gradient\n",
    "        return gradient(funcs, point, h)\n",
    "    \n",
    "    jacobian = np.zeros((n_outputs, n_inputs))\n",
    "    \n",
    "    for i, func_component in enumerate(funcs):\n",
    "        for j in range(n_inputs):\n",
    "            point_plus = point.copy()\n",
    "            point_minus = point.copy()\n",
    "            point_plus[j] += h\n",
    "            point_minus[j] -= h\n",
    "            \n",
    "            jacobian[i, j] = (func_component(point_plus) - func_component(point_minus)) / (2 * h)\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# Example: f(x, y) = [x^2 + y, x + y^2]\n",
    "def vector_func(vars):\n",
    "    x, y = vars\n",
    "    return np.array([x**2 + y, x + y**2])\n",
    "\n",
    "# The Jacobian would be:\n",
    "# [∂f1/∂x  ∂f1/∂y] = [2x  1]\n",
    "# [∂f2/∂x  ∂f2/∂y]   [1   2y]\n",
    "\n",
    "point = [1, 2]\n",
    "x, y = point\n",
    "\n",
    "print(f\"Vector function: f(x,y) = [x^2 + y, x + y^2]\")\n",
    "print(f\"At point ({x}, {y}):\")\n",
    "\n",
    "# Calculate Jacobian numerically\n",
    "jac_numerical = jacobian_vector_func(vector_func, point)\n",
    "\n",
    "# Analytical Jacobian\n",
    "jac_analytical = np.array([[2*x, 1], [1, 2*y]])\n",
    "\n",
    "print(f\"Numerical Jacobian:\\n{jac_numerical}\")\n",
    "print(f\"Analytical Jacobian:\\n{jac_analytical}\")\n",
    "print(f\"Difference: {np.linalg.norm(jac_numerical - jac_analytical):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "1. Basic derivatives and numerical computation\n",
    "2. Partial derivatives for multivariable functions\n",
    "3. Gradients and their role in optimization\n",
    "4. The chain rule and its importance in ML (backpropagation)\n",
    "5. Optimization using gradients (gradient descent)\n",
    "6. Higher-order derivatives and their interpretation\n",
    "7. Jacobian matrices for vector-valued functions\n",
    "\n",
    "Calculus is fundamental to machine learning, especially for optimization algorithms like gradient descent that power neural network training. Understanding these concepts helps in developing and debugging ML models effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}