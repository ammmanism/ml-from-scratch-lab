{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_eigenvalues_svd.ipynb\n",
    "\n",
    "## From First Principles: Eigenvalues, Eigenvectors, and Singular Value Decomposition\n",
    "\n",
    "This notebook explores two fundamental matrix decompositions that are crucial in machine learning: eigenvalue decomposition and singular value decomposition (SVD). We'll implement both from scratch and explore their applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eigenvalues and Eigenvectors\n",
    "\n",
    "An eigenvector of a square matrix A is a non-zero vector v that, when multiplied by A, yields a scalar multiple of itself:\n",
    "\n",
    "$Av = \\lambda v$\n",
    "\n",
    "where $\\lambda$ is the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: find eigenvalues and eigenvectors\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "\n",
    "# Using NumPy\n",
    "eigenvals, eigenvecs = np.linalg.eig(A)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Eigenvalues: {eigenvals}\")\n",
    "print(f\"Eigenvectors (columns):\\n{eigenvecs}\")\n",
    "\n",
    "# Verify: Av = λv\n",
    "for i in range(len(eigenvals)):\n",
    "    lambda_i = eigenvals[i]\n",
    "    v_i = eigenvecs[:, i]\n",
    "    \n",
    "    Av = A @ v_i\n",
    "    lambda_v = lambda_i * v_i\n",
    "    \n",
    "    print(f\"\\nFor eigenvalue {lambda_i:.3f} and eigenvector {v_i}:\\n\")\n",
    "    print(f\"A*v = {Av}\")\n",
    "    print(f\"λ*v = {lambda_v}\")\n",
    "    print(f\"Equal? {np.allclose(Av, lambda_v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Iteration Method\n",
    "\n",
    "A simple algorithm to find the dominant eigenvalue and its corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(A, num_iterations=100, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Find the dominant eigenvalue and eigenvector using power iteration.\n",
    "    \n",
    "    Args:\n",
    "        A: Square matrix\n",
    "        num_iterations: Maximum number of iterations\n",
    "        tolerance: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        eigenvalue: Dominant eigenvalue\n",
    "        eigenvector: Corresponding eigenvector\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    # Random initial vector\n",
    "    b_k = np.random.rand(n)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Calculate Ab\n",
    "        b_k1 = np.dot(A, b_k)\n",
    "        \n",
    "        # Normalize\n",
    "        b_k1_norm = np.linalg.norm(b_k1)\n",
    "        b_k_new = b_k1 / b_k1_norm\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(b_k, b_k_new, atol=tolerance) or np.allclose(b_k, -b_k_new, atol=tolerance):\n",
    "            break\n",
    "            \n",
    "        b_k = b_k_new\n",
    "    \n",
    "    # Calculate eigenvalue using Rayleigh quotient\n",
    "    eigenvalue = (b_k.T @ A @ b_k) / (b_k.T @ b_k)\n",
    "    \n",
    "    return eigenvalue, b_k\n",
    "\n",
    "# Test power iteration\n",
    "dominant_eigenval, dominant_eigenvec = power_iteration(A)\n",
    "\n",
    "print(f\"Power iteration result:\")\n",
    "print(f\"Dominant eigenvalue: {dominant_eigenval:.6f}\")\n",
    "print(f\"Corresponding eigenvector: {dominant_eigenvec}\")\n",
    "\n",
    "# Compare with numpy result\n",
    "print(f\"\\nNumPy result:\")\n",
    "idx = np.argmax(np.abs(eigenvals))\n",
    "print(f\"Dominant eigenvalue: {eigenvals[idx]:.6f}\")\n",
    "print(f\"Corresponding eigenvector: {eigenvecs[:, idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition\n",
    "\n",
    "For a diagonalizable matrix A, we can write: $A = V\\Lambda V^{-1}$\n",
    "\n",
    "Where V is the matrix of eigenvectors and Λ is the diagonal matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalue decomposition\n",
    "V = eigenvecs\n",
    "Lambda = np.diag(eigenvals)\n",
    "\n",
    "# Reconstruct original matrix\n",
    "A_reconstructed = V @ Lambda @ np.linalg.inv(V)\n",
    "\n",
    "print(f\"Original matrix A:\\n{A}\")\n",
    "print(f\"Reconstructed A = V @ Λ @ V^(-1):\\n{A_reconstructed}\")\n",
    "print(f\"Reconstruction accurate? {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD decomposes any matrix A into: $A = UΣV^T$\n",
    "\n",
    "Where:\n",
    "- U and V are orthogonal matrices\n",
    "- Σ is a diagonal matrix of singular values\n",
    "\n",
    "SVD works for any matrix (not just square), making it more general than eigenvalue decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD example\n",
    "A = np.array([[3, 2, 2],\n",
    "              [2, 3, -2]])\n",
    "\n",
    "# Compute SVD\n",
    "U, sigma, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Note: sigma is returned as a 1D array of singular values\n",
    "Sigma = np.zeros((A.shape[0], A.shape[1]))\n",
    "np.fill_diagonal(Sigma, sigma)\n",
    "\n",
    "print(f\"Original matrix A (shape: {A.shape}):\\n{A}\")\n",
    "print(f\"U (shape: {U.shape}):\\n{U}\")\n",
    "print(f\"Singular values: {sigma}\")\n",
    "print(f\"Σ (shape: {Sigma.shape}):\\n{Sigma}\")\n",
    "print(f\"V^T (shape: {Vt.shape}):\\n{Vt}\")\n",
    "\n",
    "# Reconstruct original matrix\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "\n",
    "print(f\"\\nReconstructed A = U @ Σ @ V^T:\\n{A_reconstructed}\")\n",
    "print(f\"Reconstruction accurate? {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship Between SVD and Eigenvalue Decomposition\n",
    "\n",
    "For a matrix A, the left singular vectors (U) are the eigenvectors of $AA^T$, and the right singular vectors (V) are the eigenvectors of $A^TA$. The singular values are the square roots of the eigenvalues of $AA^T$ or $A^TA$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the relationship\n",
    "A = np.array([[4, 0],\n",
    "              [3, -5]])\n",
    "\n",
    "# Compute SVD\n",
    "U, sigma, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Compute eigenvalues/eigenvectors of AAT and ATA\n",
    "AAT = A @ A.T\n",
    "ATA = A.T @ A\n",
    "\n",
    "eigvals_AAT, eigvecs_AAT = np.linalg.eig(AAT)\n",
    "eigvals_ATA, eigvecs_ATA = np.linalg.eig(ATA)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "idx_AAT = np.argsort(eigvals_AAT)[::-1]\n",
    "idx_ATA = np.argsort(eigvals_ATA)[::-1]\n",
    "\n",
    "eigvals_AAT = eigvals_AAT[idx_AAT]\n",
    "eigvecs_AAT = eigvecs_AAT[:, idx_AAT]\n",
    "eigvals_ATA = eigvals_ATA[idx_ATA]\n",
    "eigvecs_ATA = eigvecs_ATA[:, idx_ATA]\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"\\nSVD - Left singular vectors (U):\\n{U}\")\n",
    "print(f\"SVD - Right singular vectors (V^T):\\n{Vt}\")\n",
    "print(f\"SVD - Singular values: {sigma}\")\n",
    "\n",
    "print(f\"\\nEigenvectors of A*A^T:\\n{eigvecs_AAT}\")\n",
    "print(f\"Eigenvectors of A^T*A:\\n{eigvecs_ATA}\")\n",
    "\n",
    "print(f\"\\nSqrt of eigenvalues of A*A^T: {np.sqrt(np.abs(eigvals_AAT))}\")\n",
    "print(f\"Sqrt of eigenvalues of A^T*A: {np.sqrt(np.abs(eigvals_ATA))}\")\n",
    "print(f\"Singular values from SVD: {sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applications in Machine Learning\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA uses SVD to reduce dimensionality while preserving most of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_manual(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Perform PCA using SVD.\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (samples x features)\n",
    "        n_components: Number of principal components to keep\n",
    "        \n",
    "    Returns:\n",
    "        X_transformed: Transformed data\n",
    "        components: Principal components (loadings)\n",
    "        explained_variance_ratio: Explained variance ratio for each component\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Principal components are the rows of Vt\n",
    "    components = Vt[:n_components]\n",
    "    \n",
    "    # Transform the data\n",
    "    X_transformed = U[:, :n_components] * sigma[:n_components]\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    explained_variance = (sigma ** 2) / (X.shape[0] - 1)\n",
    "    total_variance = np.sum(explained_variance)\n",
    "    explained_variance_ratio = explained_variance / total_variance\n",
    "    \n",
    "    return X_transformed, components, explained_variance_ratio[:n_components]\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "mean = [1, 2]\n",
    "cov = [[1, 0.8], [0.8, 1]]\n",
    "X = np.random.multivariate_normal(mean, cov, 100)\n",
    "\n",
    "# Apply PCA\n",
    "X_pca, components, var_ratio = pca_manual(X, n_components=2)\n",
    "\n",
    "print(f\"Original data shape: {X.shape}\")\n",
    "print(f\"Transformed data shape: {X_pca.shape}\")\n",
    "print(f\"Principal components:\\n{components}\")\n",
    "print(f\"Explained variance ratio: {var_ratio}\")\n",
    "print(f\"Total variance explained: {np.sum(var_ratio):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# PCA transformed data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n",
    "plt.title('PCA Transformed Data')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Rank and Low-Rank Approximation\n",
    "\n",
    "SVD can be used to find the rank of a matrix and create low-rank approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approximation(A, rank):\n",
    "    \"\"\"\n",
    "    Create a low-rank approximation of matrix A using SVD.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        rank: Desired rank of approximation\n",
    "        \n",
    "    Returns:\n",
    "        A_approx: Low-rank approximation\n",
    "    \"\"\"\n",
    "    U, sigma, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Keep only the top 'rank' singular values\n",
    "    sigma_truncated = np.zeros_like(sigma)\n",
    "    sigma_truncated[:rank] = sigma[:rank]\n",
    "    \n",
    "    # Reconstruct using truncated SVD\n",
    "    A_approx = U @ np.diag(sigma_truncated) @ Vt\n",
    "    \n",
    "    return A_approx\n",
    "\n",
    "# Create a sample matrix\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "print(f\"Original matrix A:\\n{A}\")\n",
    "print(f\"Rank of A: {np.linalg.matrix_rank(A)}\")\n",
    "\n",
    "# Create low-rank approximations\n",
    "for r in [1, 2, 3]:\n",
    "    A_approx = low_rank_approximation(A, rank=r)\n",
    "    error = np.linalg.norm(A - A_approx, 'fro')  # Frobenius norm\n",
    "    print(f\"\\nRank-{r} approximation:\")\n",
    "    print(f\"Approximation:\\n{A_approx}\")\n",
    "    print(f\"Reconstruction error (Frobenius norm): {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications Beyond PCA\n",
    "\n",
    "SVD has many other applications in machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Image compression using SVD\n",
    "\n",
    "# Create a simple \"image\" (matrix)\n",
    "def create_image():\n",
    "    img = np.zeros((20, 20))\n",
    "    # Add some patterns\n",
    "    img[5:15, 5:15] = 1  # Square\n",
    "    img[2:8, 2:8] = 0.5  # Smaller square\n",
    "    return img\n",
    "\n",
    "original_img = create_image()\n",
    "\n",
    "# Apply SVD for compression\n",
    "U, sigma, Vt = np.linalg.svd(original_img)\n",
    "\n",
    "# Show how much of the energy is captured by top components\n",
    "cumulative_energy = np.cumsum(sigma**2) / np.sum(sigma**2)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(original_img, cmap='gray', interpolation='none')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Singular values\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(sigma, 'o-')\n",
    "plt.title('Singular Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Cumulative energy\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(cumulative_energy, 'o-')\n",
    "plt.title('Cumulative Energy')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Energy Captured')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstructed image with few components\n",
    "k = 5\n",
    "img_compressed = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img_compressed, cmap='gray', interpolation='none')\n",
    "plt.title(f'Compressed (k={k})')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "compression_error = np.linalg.norm(original_img - img_compressed, 'fro') / np.linalg.norm(original_img, 'fro')\n",
    "print(f\"Compression with {k} components captures {(cumulative_energy[k-1]*100):.2f}% of energy\")\n",
    "print(f\"Relative reconstruction error: {compression_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "1. Eigenvalues and eigenvectors: definitions, computation, and properties\n",
    "2. Power iteration method for finding dominant eigenpairs\n",
    "3. Singular Value Decomposition (SVD): definition and computation\n",
    "4. Relationship between SVD and eigenvalue decomposition\n",
    "5. Applications in machine learning: PCA, matrix rank, low-rank approximation, image compression\n",
    "\n",
    "These decompositions are fundamental to understanding many machine learning algorithms, particularly those related to dimensionality reduction, matrix factorization, and understanding the structure of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}