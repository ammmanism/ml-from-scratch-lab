{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_probability_distributions.ipynb\n",
    "\n",
    "## From First Principles: Probability Distributions and Their Properties\n",
    "\n",
    "This notebook explores fundamental probability distributions, their properties, and applications in machine learning. We'll implement key distributions from scratch and visualize their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gamma, factorial\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discrete Probability Distributions\n",
    "\n",
    "Discrete distributions model outcomes that take on distinct, countable values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution\n",
    "\n",
    "The Bernoulli distribution models a single trial with two possible outcomes (success/failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_pmf(k, p):\n",
    "    \"\"\"\n",
    "    Probability mass function for Bernoulli distribution.\n",
    "    \n",
    "    Args:\n",
    "        k: Outcome (0 or 1)\n",
    "        p: Probability of success\n",
    "        \n",
    "    Returns:\n",
    "        Probability of outcome k\n",
    "    \"\"\"\n",
    "    if k == 1:\n",
    "        return p\n",
    "    elif k == 0:\n",
    "        return 1 - p\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bernoulli_sample(p, size=1):\n",
    "    \"\"\"Generate samples from Bernoulli distribution\"\"\"\n",
    "    return (np.random.random(size) < p).astype(int)\n",
    "\n",
    "# Example: Fair coin flip (p=0.5)\n",
    "p = 0.5\n",
    "k_values = [0, 1]\n",
    "pmf_values = [bernoulli_pmf(k, p) for k in k_values]\n",
    "\n",
    "print(f\"Bernoulli distribution (p={p}):\")\n",
    "for k, prob in zip(k_values, pmf_values):\n",
    "    print(f\"P(X = {k}) = {prob}\")\n",
    "\n",
    "# Generate samples\n",
    "samples = bernoulli_sample(p, size=1000)\n",
    "print(f\"Sample mean: {np.mean(samples):.3f} (expected: {p})\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(k_values, pmf_values, tick_label=k_values)\n",
    "plt.title(f'Bernoulli PMF (p={p})')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(samples, bins=np.arange(3)-0.5, density=True, alpha=0.7, edgecolor='black')\n",
    "plt.title(f'Bernoulli Samples (n=1000)')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution\n",
    "\n",
    "The binomial distribution models the number of successes in n independent Bernoulli trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_coefficient(n, k):\n",
    "    \"\"\"Calculate binomial coefficient 'n choose k'\"\"\"\n",
    "    if k > n or k < 0:\n",
    "        return 0\n",
    "    if k == 0 or k == n:\n",
    "        return 1\n",
    "    \n",
    "    # Use the more efficient calculation to avoid large factorials\n",
    "    result = 1\n",
    "    for i in range(min(k, n - k)):\n",
    "        result = result * (n - i) // (i + 1)\n",
    "    return result\n",
    "\n",
    "def binomial_pmf(k, n, p):\n",
    "    \"\"\"\n",
    "    Probability mass function for binomial distribution.\n",
    "    \n",
    "    Args:\n",
    "        k: Number of successes\n",
    "        n: Number of trials\n",
    "        p: Probability of success in each trial\n",
    "        \n",
    "    Returns:\n",
    "        Probability of k successes in n trials\n",
    "    \"\"\"\n",
    "    if k < 0 or k > n:\n",
    "        return 0\n",
    "    \n",
    "    coeff = binomial_coefficient(n, k)\n",
    "    return coeff * (p ** k) * ((1 - p) ** (n - k))\n",
    "\n",
    "def binomial_sample(n, p, size=1):\n",
    "    \"\"\"Generate samples from binomial distribution\"\"\"\n",
    "    return np.sum(np.random.random((size, n)) < p, axis=1)\n",
    "\n",
    "# Example: 10 coin flips, p=0.3\n",
    "n = 10\n",
    "p = 0.3\n",
    "k_values = list(range(n + 1))\n",
    "pmf_values = [binomial_pmf(k, n, p) for k in k_values]\n",
    "\n",
    "print(f\"Binomial distribution (n={n}, p={p}):\")\n",
    "print(f\"Mean: {n*p}, Variance: {n*p*(1-p)}\")\n",
    "\n",
    "# Generate samples\n",
    "samples = binomial_sample(n, p, size=1000)\n",
    "print(f\"Sample mean: {np.mean(samples):.3f} (expected: {n*p})\")\n",
    "print(f\"Sample var: {np.var(samples):.3f} (expected: {n*p*(1-p)})\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(k_values, pmf_values)\n",
    "plt.title(f'Binomial PMF (n={n}, p={p})')\n",
    "plt.xlabel('Number of Successes')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(samples, bins=np.arange(n+2)-0.5, density=True, alpha=0.7, edgecolor='black')\n",
    "plt.title(f'Binomial Samples (n=1000)')\n",
    "plt.xlabel('Number of Successes')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continuous Probability Distributions\n",
    "\n",
    "Continuous distributions model outcomes that can take any value in a continuous range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal (Gaussian) Distribution\n",
    "\n",
    "The normal distribution is fundamental in statistics and machine learning due to the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu=0, sigma=1):\n",
    "    \"\"\"\n",
    "    Probability density function for normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        x: Point at which to evaluate the PDF\n",
    "        mu: Mean\n",
    "        sigma: Standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        PDF value at x\n",
    "    \"\"\"\n",
    "    coefficient = 1 / (sigma * np.sqrt(2 * np.pi))\n",
    "    exponent = -0.5 * ((x - mu) / sigma) ** 2\n",
    "    return coefficient * np.exp(exponent)\n",
    "\n",
    "def normal_sample(mu=0, sigma=1, size=1):\n",
    "    \"\"\"Generate samples from normal distribution using Box-Muller transform\"\"\"\n",
    "    # Using numpy's random for simplicity, but could implement Box-Muller\n",
    "    return np.random.normal(mu, sigma, size)\n",
    "\n",
    "# Example: Normal distribution with μ=0, σ=1\n",
    "mu, sigma = 0, 1\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "pdf_values = normal_pdf(x, mu, sigma)\n",
    "\n",
    "print(f\"Normal distribution (μ={mu}, σ={sigma}):\")\n",
    "print(f\"Mean: {mu}, Variance: {sigma**2}\")\n",
    "\n",
    "# Generate samples\n",
    "samples = normal_sample(mu, sigma, size=1000)\n",
    "print(f\"Sample mean: {np.mean(samples):.3f} (expected: {mu})\")\n",
    "print(f\"Sample std: {np.std(samples):.3f} (expected: {sigma})\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, pdf_values, linewidth=2)\n",
    "plt.title(f'Normal PDF (μ={mu}, σ={sigma})')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "plt.plot(x, pdf_values, 'r-', linewidth=2, label='True PDF')\n",
    "plt.title(f'Normal Samples vs True PDF (n=1000)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Distribution\n",
    "\n",
    "The exponential distribution models the time between events in a Poisson process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_pdf(x, lambd):\n",
    "    \"\"\"\n",
    "    Probability density function for exponential distribution.\n",
    "    \n",
    "    Args:\n",
    "        x: Point at which to evaluate the PDF (x >= 0)\n",
    "        lambd: Rate parameter (lambda > 0)\n",
    "        \n",
    "    Returns:\n",
    "        PDF value at x\n",
    "    \"\"\"\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    return lambd * np.exp(-lambd * x)\n",
    "\n",
    "def exponential_sample(lambd, size=1):\n",
    "    \"\"\"Generate samples from exponential distribution using inverse transform sampling\"\"\"\n",
    "    u = np.random.uniform(0, 1, size)\n",
    "    return -np.log(1 - u) / lambd\n",
    "\n",
    "# Example: Exponential distribution with λ=0.5\n",
    "lambd = 0.5\n",
    "x = np.linspace(0, 10, 1000)\n",
    "pdf_values = [exponential_pdf(xi, lambd) for xi in x]\n",
    "\n",
    "print(f\"Exponential distribution (λ={lambd}):\")\n",
    "print(f\"Mean: {1/lambd}, Variance: {1/lambd**2}\")\n",
    "\n",
    "# Generate samples\n",
    "samples = exponential_sample(lambd, size=1000)\n",
    "print(f\"Sample mean: {np.mean(samples):.3f} (expected: {1/lambd})\")\n",
    "print(f\"Sample var: {np.var(samples):.3f} (expected: {1/lambd**2})\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, pdf_values, linewidth=2)\n",
    "plt.title(f'Exponential PDF (λ={lambd})')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "plt.plot(x, pdf_values, 'r-', linewidth=2, label='True PDF')\n",
    "plt.title(f'Exponential Samples vs True PDF (n=1000)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multivariate Distributions\n",
    "\n",
    "Multivariate distributions model multiple random variables simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal Distribution\n",
    "\n",
    "The multivariate normal distribution is the generalization of the univariate normal to multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_pdf(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Probability density function for multivariate normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        x: Point at which to evaluate the PDF (array-like)\n",
    "        mu: Mean vector\n",
    "        sigma: Covariance matrix\n",
    "        \n",
    "    Returns:\n",
    "        PDF value at x\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    mu = np.asarray(mu)\n",
    "    sigma = np.asarray(sigma)\n",
    "    \n",
    "    if x.shape != mu.shape:\n",
    "        raise ValueError(\"x and mu must have the same shape\")\n",
    "    \n",
    "    d = len(mu)  # dimension\n",
    "    \n",
    "    # Calculate determinant and inverse of covariance matrix\n",
    "    det_sigma = np.linalg.det(sigma)\n",
    "    if det_sigma <= 0:\n",
    "        raise ValueError(\"Covariance matrix must be positive definite\")\n",
    "    \n",
    "    inv_sigma = np.linalg.inv(sigma)\n",
    "    \n",
    "    # Calculate the exponent\n",
    "    diff = x - mu\n",
    "    exponent = -0.5 * diff.T @ inv_sigma @ diff\n",
    "    \n",
    "    # Calculate the normalization constant\n",
    "    norm_const = 1.0 / np.sqrt((2 * np.pi) ** d * det_sigma)\n",
    "    \n",
    "    return norm_const * np.exp(exponent)\n",
    "\n",
    "def multivariate_normal_sample(mu, sigma, size=1):\n",
    "    \"\"\"Generate samples from multivariate normal distribution\"\"\"\n",
    "    return np.random.multivariate_normal(mu, sigma, size)\n",
    "\n",
    "# Example: 2D multivariate normal\n",
    "mu = np.array([0, 1])\n",
    "sigma = np.array([[1, 0.5],\n",
    "                  [0.5, 1]])\n",
    "\n",
    "# Generate samples\n",
    "samples = multivariate_normal_sample(mu, sigma, size=1000)\n",
    "\n",
    "print(f\"2D Multivariate Normal (μ=[{mu[0]}, {mu[1]}]):\")\n",
    "print(f\"Covariance matrix:\\n{sigma}\")\n",
    "print(f\"Sample mean: [{np.mean(samples[:, 0]):.3f}, {np.mean(samples[:, 1]):.3f}] (expected: [{mu[0]}, {mu[1]}])\")\n",
    "print(f\"Sample covariance:\\n{np.cov(samples.T)}\")\n",
    "\n",
    "# Create grid for contour plot\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-3, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "# Calculate PDF values\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = multivariate_normal_pdf(pos[i, j], mu, sigma)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contour(X, Y, Z, levels=10)\n",
    "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.6, s=10)\n",
    "plt.title('2D Multivariate Normal Samples + Contours')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.6)\n",
    "plt.title('2D Multivariate Normal Samples')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Fitting and Parameter Estimation\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a common approach to estimate distribution parameters from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_normal_params(data):\n",
    "    \"\"\"\n",
    "    Calculate MLE parameters for normal distribution.\n",
    "    For normal distribution, MLE estimates are simply the sample mean and variance.\n",
    "    \"\"\"\n",
    "    mu_mle = np.mean(data)\n",
    "    sigma_mle = np.sqrt(np.mean((data - mu_mle)**2))  # biased estimator\n",
    "    return mu_mle, sigma_mle\n",
    "\n",
    "def mle_exponential_param(data):\n",
    "    \"\"\"\n",
    "    Calculate MLE parameter for exponential distribution.\n",
    "    For exponential distribution, MLE estimate is 1 / sample_mean.\n",
    "    \"\"\"\n",
    "    return 1 / np.mean(data)\n",
    "\n",
    "# Test MLE estimation\n",
    "print(\"Testing MLE parameter estimation:\")\n",
    "\n",
    "# Normal distribution\n",
    "true_mu, true_sigma = 2, 1.5\n",
    "normal_data = np.random.normal(true_mu, true_sigma, 1000)\n",
    "est_mu, est_sigma = mle_normal_params(normal_data)\n",
    "\n",
    "print(f\"Normal distribution:\")\n",
    "print(f\"  True params: μ={true_mu}, σ={true_sigma}\")\n",
    "print(f\"  Estimated: μ={est_mu:.3f}, σ={est_sigma:.3f}\")\n",
    "\n",
    "# Exponential distribution\n",
    "true_lambda = 0.8\n",
    "exp_data = np.random.exponential(1/true_lambda, 1000)  # numpy uses scale = 1/lambda\n",
    "est_lambda = mle_exponential_param(exp_data)\n",
    "\n",
    "print(f\"\\nExponential distribution:\")\n",
    "print(f\"  True λ: {true_lambda}\")\n",
    "print(f\"  Estimated λ: {est_lambda:.3f}\")\n",
    "\n",
    "# Visualize the fitted distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(normal_data, bins=50, density=True, alpha=0.7, label='Data')\n",
    "x = np.linspace(normal_data.min(), normal_data.max(), 1000)\n",
    "plt.plot(x, normal_pdf(x, est_mu, est_sigma), 'r-', linewidth=2, label=f'Fitted N({est_mu:.2f}, {est_sigma:.2f})')\n",
    "plt.title('Normal Distribution Fitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(exp_data, bins=50, density=True, alpha=0.7, label='Data')\n",
    "x = np.linspace(0, exp_data.max(), 1000)\n",
    "exp_pdf_vals = [exponential_pdf(xi, est_lambda) for xi in x]\n",
    "plt.plot(x, exp_pdf_vals, 'r-', linewidth=2, label=f'Fitted Exp(λ={est_lambda:.2f})')\n",
    "plt.title('Exponential Distribution Fitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applications in Machine Learning\n",
    "\n",
    "Probability distributions are fundamental to many ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Naive Bayes classifier using normal distribution assumption\n",
    "\n",
    "class NaiveBayesNormal:\n",
    "    \"\"\"Naive Bayes classifier with normal distribution assumption for features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = {}\n",
    "        self.feature_params = {}  # Store (mean, std) for each feature and class\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # Calculate class prior\n",
    "            class_samples = X[y == c]\n",
    "            self.class_priors[c] = len(class_samples) / n_samples\n",
    "            \n",
    "            # Calculate feature parameters (mean, std) for each feature\n",
    "            self.feature_params[c] = {\n",
    "                'means': np.mean(class_samples, axis=0),\n",
    "                'stds': np.std(class_samples, axis=0) + 1e-6  # Add small value to avoid division by zero\n",
    "            }\n",
    "    \n",
    "    def _calculate_likelihood(self, x, mean, std):\n",
    "        \"\"\"Calculate likelihood using normal PDF\"\"\"\n",
    "        # Avoid division by zero\n",
    "        std = np.maximum(std, 1e-6)\n",
    "        exponent = -0.5 * ((x - mean) / std) ** 2\n",
    "        coefficient = 1 / (std * np.sqrt(2 * np.pi))\n",
    "        return coefficient * np.exp(exponent)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes)\n",
    "        probas = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Calculate class prior\n",
    "            log_prior = np.log(self.class_priors[c])\n",
    "            \n",
    "            # Calculate log likelihood for each feature\n",
    "            means = self.feature_params[c]['means']\n",
    "            stds = self.feature_params[c]['stds']\n",
    "            \n",
    "            # Calculate log likelihood for all samples and features\n",
    "            log_likelihood = np.sum(\n",
    "                np.log(self._calculate_likelihood(X, means, stds)),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Posterior is proportional to prior * likelihood\n",
    "            probas[:, i] = log_prior + log_likelihood\n",
    "        \n",
    "        # Convert log probabilities to probabilities and normalize\n",
    "        # Subtract max for numerical stability\n",
    "        probas = probas - np.max(probas, axis=1, keepdims=True)\n",
    "        probas = np.exp(probas)\n",
    "        probas = probas / np.sum(probas, axis=1, keepdims=True)\n",
    "        \n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes[np.argmax(probas, axis=1)]\n",
    "\n",
    "# Generate sample data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Class 0: centered at [2, 2]\n",
    "class0 = np.random.multivariate_normal([2, 2], [[1, 0.3], [0.3, 1]], n_samples//2)\n",
    "labels0 = np.zeros(n_samples//2)\n",
    "\n",
    "# Class 1: centered at [0, 0]\n",
    "class1 = np.random.multivariate_normal([0, 0], [[1, -0.2], [-0.2, 1]], n_samples//2)\n",
    "labels1 = np.ones(n_samples//2)\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack([class0, class1])\n",
    "y = np.hstack([labels0, labels1])\n",
    "\n",
    "# Split into train and test\n",
    "split_idx = int(0.7 * n_samples)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Train the model\n",
    "nb_model = NaiveBayesNormal()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "print(f\"Naive Bayes Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training data\n",
    "plt.subplot(1, 3, 1)\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Plot 2: Test data with true labels\n",
    "plt.subplot(1, 3, 2)\n",
    "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
    "plt.title('Test Data (True Labels)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Plot 3: Test data with predicted labels\n",
    "plt.subplot(1, 3, 3)\n",
    "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='viridis', alpha=0.7)\n",
    "plt.title(f'Test Data (Predictions, Acc: {accuracy:.3f})')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "1. Discrete distributions: Bernoulli, Binomial\n",
    "2. Continuous distributions: Normal, Exponential\n",
    "3. Multivariate distributions: Multivariate Normal\n",
    "4. Parameter estimation: Maximum Likelihood Estimation\n",
    "5. Applications: Naive Bayes classifier\n",
    "\n",
    "Understanding probability distributions is crucial for machine learning as they form the basis for many algorithms, from simple classifiers to complex generative models. They help us model uncertainty, make predictions, and understand the underlying patterns in data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}